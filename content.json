{"meta":{"title":"Colin's Blog","subtitle":"","description":"个人博客","author":"Colin Lin","url":"http://linco.com"},"pages":[],"posts":[{"title":"Mybatis 入门","slug":"mybatis入门01","date":"2021-05-16T07:09:00.865Z","updated":"2021-05-16T07:09:00.866Z","comments":true,"path":"2021/05/16/mybatis入门01/","link":"","permalink":"http://linco.com/2021/05/16/mybatis%E5%85%A5%E9%97%A801/","excerpt":"","text":"Mybatis简介MyBatis 前身是iBatis,其源于“Internet”和“ibatis”的组合， 是一款优秀的持久层框架，它支持自定义SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。MyBatis 可以通过简单的 XML 或注解来配置和映射原始类型、接口和 Java POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。 其本质是一种半自动的ORM 框架，除了POJO 和映射关系之外，还需要编写SQL 语句；Mybatis 映射文件三要素：SQL、映射规则和POJO为什么需要ORM框架传统的JDBC 编程存在的弊端: - 工作量大，操作数据库至少要5步； - 业务代码和技术代码耦合； - 连接资源手动关闭，带来了隐患。入门导入依赖12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.4&lt;/version&gt;&lt;/dependency&gt; 创建Mybatis配置文件文件名：mybatis-config.xml, 以下主要列出mybatis配置文件的顶层结构，详细配置可参考:mybatis.org 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!--properties属性配置--&gt; &lt;properties/&gt; &lt;!--设置--&gt; &lt;settings/&gt; &lt;!--类型别名，类型别名可为 Java 类型设置一个缩写名字。 它仅用于 XML 配置，意在降低冗余的全限定类名书写。--&gt; &lt;typeAliases/&gt; &lt;!--类型处理器--&gt; &lt;typeHandlers/&gt; &lt;!-- 对象工厂--&gt; &lt;objectFactory&gt; &lt;!-- 插件 --&gt; &lt;plugins）/&gt; &lt;!--environments 元素定义了如何配置环境。--&gt; &lt;environments default=\"development\"&gt; &lt;environment/&gt; &lt;!-- 事务管理器 --&gt; &lt;transactionManager/&gt; &lt;!--数据源--&gt; &lt;dataSource&gt; &lt;/environments&gt; &lt;!-- 映射器 --&gt; &lt;mappers&gt; &lt;mapper resource=\"sqlmapper/UserMapper.xml\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 创建 Java Bean 实体类12345678910package com.colin.mybatisdemo.entiry;public class User &#123; private Integer id; private String userName; private String mobile;&#125; 创建 Mapper 接口跟Mapper xml 文件映射 1234567package com.colin.mybatisdemo.mapper;import com.colin.mybatisdemo.entiry.User;public interface UserMapper &#123; User getOneById(int id);&#125; 创建 Mapper xml 映射文件123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.colin.mybatisdemo.mapper.UserMapper\"&gt; &lt;select id=\"getOneById\" resultType=\"User\"&gt; SELECT * from USER where id = #&#123;id, jdbcType=INTEGER&#125;; &lt;/select&gt;&lt;/mapper&gt; &emsp;&emsp;MyBatis 的真正强大在于它的语句映射，这是它的魔力所在。由于它的异常强大，映射器的 XML 文件就显得相对简单。如果拿它跟具有相同功能的 JDBC 代码进行对比，你会立即发现省掉了将近 95% 的代码。MyBatis 致力于减少使用成本，让用户能更专注于 SQL 代码。 &emsp;&emsp;SQL 映射文件只有很少的几个顶级元素（按照应被定义的顺序列出）： cache – 该命名空间的缓存配置。 cache-ref – 引用其它命名空间的缓存配置。 resultMap – 描述如何从数据库结果集中加载对象，是最复杂也是最强大的元素。 parameterMap – 老式风格的参数映射。此元素已被废弃，并可能在将来被移除！请使用行内参数映射。文档中不会介绍此元素。 sql – 可被其它语句引用的可重用语句块。 insert – 映射插入语句。 update – 映射更新语句。 delete – 映射删除语句。 select – 映射查询语句。 具体使用详解可参考：[mybatis.org](https://mybatis.org/mybatis-3/zh/sqlmap-xml.html)创建一个测试类-快速入门12345678910111213141516171819202122232425262728public class UserMapperTest &#123; private SqlSessionFactory sqlSessionFactory; @Before public void testBefore() throws IOException &#123; //--------------------第一阶段--------------------------- // 1.读取mybatis配置文件创SqlSessionFactory String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); // 1.读取mybatis配置文件创SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); inputStream.close(); &#125; @Test public void testGetOneById()&#123; //--------------------第二阶段--------------------------- // 2.获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); // 3.获取对应mapper UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //--------------------第三阶段--------------------------- // 4.执行查询语句并返回单条数据 User user = userMapper.getOneById(1); System.out.println(user.getUserName() + \":\" + user.getMobile()); &#125;&#125; 结束语Mybatis 入门介绍就讲到这里了，博客Demo源码请访问：mybatis-demo","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"MyBatis","slug":"MyBatis","permalink":"http://linco.com/tags/MyBatis/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-05-16T07:09:00.858Z","updated":"2021-05-16T07:09:00.864Z","comments":true,"path":"2021/05/16/hello-world/","link":"","permalink":"http://linco.com/2021/05/16/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"前端","slug":"前端","permalink":"http://linco.com/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[],"keywords":[{"name":"前端","slug":"前端","permalink":"http://linco.com/categories/%E5%89%8D%E7%AB%AF/"}]},{"title":"Docker 安装 Mysql8 并初始化数据库","slug":"docker安装Mysql8并初始化数据库","date":"2021-05-16T07:09:00.857Z","updated":"2021-05-16T07:09:00.857Z","comments":true,"path":"2021/05/16/docker安装Mysql8并初始化数据库/","link":"","permalink":"http://linco.com/2021/05/16/docker%E5%AE%89%E8%A3%85Mysql8%E5%B9%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"最近有个比较特殊的需求，通过Docker安装Mysql8, 并且希望容器启动的时候，能够自动化初始化数据库以及基础数据。 于是查找了相关资料之后，发现其实Mysql8官方Docker镜像中已经支持了该功能。在容器启动的时会自动执行指定目录[/docker-entrypoint-initdb.d]下的.sql脚本或者.sh脚本,具体可以查看官方Docker镜像的Dockerfile文件已经docker-entrypoint.sh文件。 因此，我们想要Mysql容器启动时就自动初始化数据库，只需将我们的.sql或者.sh复制到/docker-entrypoint-initdb.d目录下即可。 创建Dockerfile123456789101112131415161718192021FROM mysql/mysql-server:8.0#定义会被容器自动执行的目录ENV AUTO_RUN_DIR ./docker-entrypoint-initdb.d#定义初始化sql文件ENV INIT_SQL init-mydb.sqlENV INIT_PRIVILEGES_SQL privileges.sql#定义初始化sh脚本ENV INIT_SH setup.sh#把执行的sql文件放到docker-entrypoint-initdb.d/目录下，容器会自动执行这个sqlCOPY ./$INIT_SQL ./COPY ./$INIT_PRIVILEGES_SQL ./COPY ./$INIT_SH $AUTO_RUN_DIR/#RUN chmod a+x $AUTO_RUN_DIR/$INIT_SQL#RUN chmod a+x $AUTO_RUN_DIR/$INIT_SQLRUN chmod a+x $AUTO_RUN_DIR/$INIT_SHEXPOSE 3306 准备数据库初始化sql文件创建数据库 init-mydb.sql:12345678910111213141516CREATE DATABASE IF NOT EXISTS `test`;USE `test`; SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;CREATE TABLE `user` ( `userId` varchar(255) NOT NULL, `createTime` datetime DEFAULT NULL, `updateBy` varchar(255) DEFAULT NULL, `updateTime` datetime DEFAULT NULL, `username` varchar(255) NOT NULL, PRIMARY KEY (`userId`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;SET FOREIGN_KEY_CHECKS = 1; 数据库授权 privileges.sql12345use mysql;-- 将test数据库的权限授权给创建的test_owner用户grant all privileges on test.* to 'test_owner'@'%';-- 这一条命令一定要有：flush privileges; 准备初始化执行的sh文件1234567#!/bin/bashecho '初始化DB'mysql -uroot -p$MYSQL_ROOT_PASSWORD &lt;&lt;EOF source ./init-mydb.sql; source ./privileges.sql;EOF 使用docker-compose.yml文件创建容器12345678910111213141516171819202122version: '2'services: int-mysql-server: image: docker-mysql-init:latest container_name: int-mysql-server environment: MYSQL_ROOT_PASSWORD: 'root123456' # root 账号的密码 MYSQL_DATABASE: 'testdb' # 默认创建的数据库名 MYSQL_USER: 'testdb_owner' # MYSQL_DATABASE 对应的用户名 MYSQL_PASSWORD: 'test123456' # MYSQL_DATABASE 对应的用户密码 MYSQL_ROOT_HOST: '%' # 运行root 远程登录 restart: always ports: - \"3306:3306\" logging: driver: json-file options: max-size: 20m max-file: '5' labels: io.rancher.container.pull_image: always io.rancher.scheduler.global: 'true' 运行准备好以上文件之后，就可以使用docker build 命令构建docker镜像： 1docker build -t docker-mysql-init:latest . 使用 docker-compose 命令运行容器： 1docker-compose up -d 注：非第一次部署如果需要再次执行初始化sql, 则需要把Container挂载卷删除之后再重新部署，否则初始化SQL和SH不会再次执行。","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"Dockerfile 相关命令详解","slug":"dockerfile相关命令详解","date":"2021-05-16T07:09:00.856Z","updated":"2021-05-16T07:09:00.856Z","comments":true,"path":"2021/05/16/dockerfile相关命令详解/","link":"","permalink":"http://linco.com/2021/05/16/dockerfile%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Dockerfile&emsp;&emsp;镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。 &emsp;&emsp;Dockerfile 是一个文本文件，其内包含了一条条的 指令(Instruction)，每一条指令构建镜像的一层，因此每一条指令的内容，就是描述该镜像层应当如何构建。 Dockerfile的基本结构：&emsp;&emsp;Dockerfile 一般分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令，’#’ 为 Dockerfile 中的注释。 Docker 命令详解 FROM 指定基础镜像一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。格式： 1234FROM &lt;image&gt;FROM &lt;image&gt;:&lt;tag&gt;FROM &lt;image&gt;@&lt;digest&gt;#tag或digest是可选的，如果不使用这两个值时，会使用latest版本的基础镜像 示例： FROM nginx:latestMAINTAINER 维护者信息，可选格式： 1MAINTAINER &lt;name&gt; 示例： MAINTAINER ColinLinRUN 执行命令RUN用于在镜像容器中执行命令，其有以下两种命令执行方式：shell 格式： 1RUN &lt;命令&gt; #就像直接在命令行中输入的命令一样。 示例： RUN apt-get update exec 格式： 1RUN [\"可执行文件\", \"参数1\", \"参数2\"] #这更像是函数调用中的格式。 示例： RUN [&quot;/etc/execfile&quot;, &quot;arg1&quot;, &quot;arg1&quot;]注：如果需要将两条命令或者多条命令联合起来执行需要加上&amp;，例如：cd /usr/local/src &amp;&amp; wget xxxxxxx COPY 复制文件RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用 12COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径&gt;... &lt;目标路径&gt;COPY [--chown=&lt;user&gt;:&lt;group&gt;] [\"&lt;源路径1&gt;\",... \"&lt;目标路径&gt;\"] COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。示例： COPY package.json /usr/src/app/&lt;源路径&gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则。示例： COPY hom* /mydir/ COPY hom?.txt /mydir/&lt;目标路径&gt; 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 ADD 更高级的复制文件ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。ADD命令会将tar类型文件会自动解压(网络压缩资源不会被解压)，可以访问网络资源，类似wget,而copy则不能。 示例： ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /注：在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 CMD 容器启动命令，也就是构建容器后才调用CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式： 1CMD &lt;命令&gt; exec 格式： 12CMD [\"可执行文件\", \"参数1\", \"参数2\"...]#参数列表格式：CMD [\"参数1\", \"参数2\"...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 注：当运行container 的时候声明了command，则不再用image 中的CMD 默认所定义的命令。一个Dockerfile 中只能有一个有效的CMD，当定义多个CMD 的时候，只有最后一个才会起作用 ENTRYPOINTENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 –entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： 1&lt;ENTRYPOINT&gt; \"&lt;CMD&gt;\" 关于 ENTRYPOINT CMD的使用场景，可以参考gitbooks 注：ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令。 ENV 设置环境变量格式： 12ENV &lt;key&gt; &lt;value&gt; #&lt;key&gt;之后的所有内容均会被视为其&lt;value&gt;的组成部分，因此，一次只能设置一个变量ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...#可以设置多个变量，每个变量为一个\"&lt;key&gt;=&lt;value&gt;\"的键值对，如果&lt;key&gt;中包含空格，可以使用\\来进行转义，也可以通过\"\"来进行标示；另外，反斜线也可以用于续行 示例： ENV MY_NAME Colin Lin ENV VERSION=1.0 DEBUG=on \\ NAME=&quot;Happy Feet&quot;ARG 用于指定传递给构建运行时的变量Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 –build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。 格式： 1ARG &lt;参数名&gt;[=&lt;默认值&gt;] 示例： ARG build_user=wwwVOLUME 定义匿名卷（指定持久化目录）格式： 12VOLUME [\"&lt;路径1&gt;\", \"&lt;路径2&gt;\"...]VOLUME &lt;路径&gt; 一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能：1 卷可以容器间共享和重用2 容器并不一定要和其它容器共享卷3 修改卷后会立即生效4 对卷的修改不会对镜像产生影响5 卷会一直存在，直到没有任何容器在使用它 示例： VOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：docker run -d -v mydata:/data xxxx在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 EXPOSE 暴露端口格式： 1EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...]。 EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 WORKDIR 指定工作目录使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。格式： 1WORKDIR &lt;工作目录路径&gt;。 USER 指定当前用户格式： 1USER &lt;用户名&gt;[:&lt;用户组&gt;] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。注：USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 示例： RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis USER redis RUN [ &quot;redis-server&quot; ]如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。示例： # 建立 redis 用户，并使用 gosu 换另一个用户执行命令 RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis # 下载 gosu RUN wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64&quot; \\ &amp;&amp; chmod +x /usr/local/bin/gosu \\ &amp;&amp; gosu nobody true # 设置 CMD，并以另外的用户执行 CMD [ &quot;exec&quot;, &quot;gosu&quot;, &quot;redis&quot;, &quot;redis-server&quot; ]HEALTHCHECK 健康检查格式： 12HEALTHCHECK [选项] CMD &lt;命令&gt; #设置检查容器健康状况的命令HEALTHCHECK NONE #如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy HEALTHCHECK 支持下列选项：–interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒；–timeout=&lt;时长&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒；–retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。 示例： FROM nginx RUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; rm -rf /var/lib/apt/lists/* HEALTHCHECK --interval=5s --timeout=3s \\ CMD curl -fs http://localhost/ || exit 1ONBUILD 为他人做嫁衣裳格式： 1ONBUILD &lt;其它指令&gt;。 ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"docker 容器内部安装 vim","slug":"docker 容器内部安装 vim","date":"2021-05-16T07:09:00.855Z","updated":"2021-05-16T07:09:00.855Z","comments":true,"path":"2021/05/16/docker 容器内部安装 vim/","link":"","permalink":"http://linco.com/2021/05/16/docker%20%E5%AE%B9%E5%99%A8%E5%86%85%E9%83%A8%E5%AE%89%E8%A3%85%20vim/","excerpt":"","text":"Docker 容器内部默认是没有按准vim的，如果我们需要在docker容器内部使用vim, 可以通过以下方式安装： 1、首先执行以下命令，更新安装源： 1apt-get update 2、再执行以下命令安装vim即可： 1apt-get install vim","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"},{"name":"后端","slug":"后端","permalink":"http://linco.com/tags/%E5%90%8E%E7%AB%AF/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"Docker Swarm 基本使用","slug":"docker Swarm基本使用","date":"2021-05-16T07:09:00.854Z","updated":"2021-05-16T07:09:00.855Z","comments":true,"path":"2021/05/16/docker Swarm基本使用/","link":"","permalink":"http://linco.com/2021/05/16/docker%20Swarm%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Swarm 简介&emsp;&emsp;Swarm 是Docker 官方提供的一款集群管理工具，其主要作用是把若干台Docker 主机抽象为一个整体。Docker Swarm 提供了标准的 Docker API，所有任何已经与 Docker 守护程序通信的工具都可以使用 Swarm 轻松地扩展到多个主机。 &emsp;&emsp;Swarm 和 Kubernetes 比较类似，但是更加轻，具有的功能也较 kubernetes 更少一些。 &emsp;&emsp;Swarm 集群由管理节点（manager）和工作节点（work node）构成。如下图所示：swarm mananger：负责整个集群的管理工作包括集群配置、服务管理等所有跟集群有关的工作。work node：即图中的 available node，主要负责运行相应的服务来执行任务（task）。 &emsp;&emsp;当你创建一个service 时，你定义了它的理想状态（副本数、网络、存储资源、对外暴露的端口等）。Docker 会维持它的状态，例如，如果一个worker node 不可用了，Docker会调度不可用node 的task 到其他nodes 上。 Swarm 环境搭建及初始化准备三台互通的机器(此处用的是虚拟机)： 宿主机名称 IP 备注 linco-centos7 192.168.31.221 manager linco-centos72 192.168.31.222 work node linco-centos73 192.168.31.223 work node 在manager主机上执行以下命令创建管理节点1docker swarm init --advertise-addr 192.168.31.221 运行结果如下： 1234567Swarm initialized: current node (kj8wmkf6mcbayasj8zqkdclht) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5u4ik4lp7b0riq5nejhvflbp84s1uhi6kpnmgf7o50ouq8prg0-dudp3uo991c6ladck13eydxbj 192.168.31.221:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 在其他子节点上执行以下命令添加工作节点到集群1docker swarm join --token SWMTKN-1-5u4ik4lp7b0riq5nejhvflbp84s1uhi6kpnmgf7o50ouq8prg0-dudp3uo991c6ladck13eydxbj 192.168.31.221:2377 结果： 1This node joined a swarm as a worker. 如果出现如下错误，则需要打开2377端口，具体可参考linux使用firewall打开端口 Error response from daemon: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = &quot;transport: Error while dialing dial tcp 192.168.31.221:2377: connect: no route to host&quot;在manager主机上运行 docker node ls,可以看到其他两个节点已经成功加入12345[linco@linco-centos7 ~]$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONkj8wmkf6mcbayasj8zqkdclht * linco-centos7 Ready Active Leader 18.09.9kwys625i3mkxd1vo0z2r4z0fi linco-centos72 Ready Active 18.09.9npupwq5ee8dnmlejxmfa6mou6 linco-centos73 Ready Active 18.09.9 部署Docker 应用（此处以Nginx为例）&emsp;&emsp;Swarm 环境搭建好之后，整个工作节点集群，都可以当作一个宿主机来操作了。可以在manager 上进行各种服务的部署管理 创建服务运行以下命令，创建一个nginx 集群，3 台机器 1docker service create --name nginx --replicas 3 -p 80:80 nginx:1.7.9 执行docker service ls命令，可以查看服务列表： 123[linco@linco-centos7 ~]$ docker service lsID NAME MODE REPLICAS IMAGE PORTSt6fh31u0erwy nginx replicated 3/3 nginx:1.7.9 *:80-&gt;80/tcp 执行docker service ps [服务名] 可以查看服务详情： 12345[linco@linco-centos7 ~]$ docker service ps nginxID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSzq8zhhu8h6ir nginx.1 nginx:1.7.9 linco-centos72 Running Running about a minute ago 8sv070ql676r nginx.2 nginx:1.7.9 linco-centos73 Running Running about a minute ago qvwhdpibe151 nginx.3 nginx:1.7.9 linco-centos7 Running Running about a minute ago 访问服务，任何一台主机ip都可以访问到： 服务维护&emsp;&emsp;服务集群运行后，会以期望的状态方式管理整个集群。如，刚刚创建的3 中nginx 组成的集群。如果其中一台宕机，swarm 会自动新建一台，来补足task 数量. 在一个节点机器上，删除一个容器: 12[linco@linco-centos73 ~]$ docker rm -fv 6340d500893c6340d500893c 在管理节点上查询可以看到，又新启了一台服务 123456[linco@linco-centos7 ~]$ docker service ps nginxID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSzq8zhhu8h6ir nginx.1 nginx:1.7.9 linco-centos72 Running Running 28 minutes ago wu3hnwbzees1 nginx.2 nginx:1.7.9 linco-centos73 Running Running 47 seconds ago 8sv070ql676r \\_ nginx.2 nginx:1.7.9 linco-centos73 Shutdown Failed 53 seconds ago \"task: non-zero exit (137)\" qvwhdpibe151 nginx.3 nginx:1.7.9 linco-centos7 Running Running 27 minutes ago 滚动升级更新原来的nginx 版本为1.9.7 1docker service update --image nginx:1.9.7 nginx 更新之后查看服务，可以看到版本已经升级了: 123[linco@linco-centos7 ~]$ docker service lsID NAME MODE REPLICAS IMAGE PORTSt6fh31u0erwy nginx replicated 3/3 nginx:1.9.7 *:80-&gt;80/tcp 动态扩容服务还可以动态在节点集群上进行扩容 1docker service scale nginx=4 再次查看，发现服务已经变为四个节点了 12345678910[linco@linco-centos7 ~]$ docker service ps nginxID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSxrv33gu16lmc nginx.1 nginx:1.9.7 linco-centos72 Running Running 3 minutes ago zq8zhhu8h6ir \\_ nginx.1 nginx:1.7.9 linco-centos72 Shutdown Shutdown 4 minutes ago x1o334p47add nginx.2 nginx:1.9.7 linco-centos73 Running Running 5 minutes ago wu3hnwbzees1 \\_ nginx.2 nginx:1.7.9 linco-centos73 Shutdown Shutdown 6 minutes ago 8sv070ql676r \\_ nginx.2 nginx:1.7.9 linco-centos73 Shutdown Failed 8 minutes ago \"task: non-zero exit (137)\" mwkvokgc6nod nginx.3 nginx:1.9.7 linco-centos7 Running Running 4 minutes ago qvwhdpibe151 \\_ nginx.3 nginx:1.7.9 linco-centos7 Shutdown Shutdown 5 minutes ago wnvqmydgh1ax nginx.4 nginx:1.9.7 linco-centos7 Running Running 12 seconds ago 停止服务使用下面命令和停止对应服务 123docker service rm hellos或docker service scale hellos=0 网络通讯swarm 管理的集群，内部建立一个统一的网络层,你绑定的端口，并非是直接绑定到宿主机，而是绑在这个overlay 网络。对于集群来说，相当于都绑在网络的一个端口上，启用负载策略来转发请求，如下图:","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"Centos7 安装指定版本 Docker","slug":"centos7安装docker","date":"2021-05-16T07:09:00.853Z","updated":"2021-05-16T07:09:00.853Z","comments":true,"path":"2021/05/16/centos7安装docker/","link":"","permalink":"http://linco.com/2021/05/16/centos7%E5%AE%89%E8%A3%85docker/","excerpt":"","text":"卸载旧版本12345678sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine Docker 默认安装1$ yum -y install docker # 默认安装最新版本 Docker安装指定版本安装所需软件包yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2。 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 使用yum-config-manager 添加docker仓库123$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 执行以下命令，列出 docker 所有可用版本1$ yum list docker-ce --showduplicates | sort -r 根据自己选择特定版本号的docker安装$ sudo yum install docker-ce- docker-ce-cli- containerd.io 例如： 1sudo yum install docker-ce-18.09.9 docker-ce-cli-18.09.9 containerd.io -y 启动 Docker 并且配置 开机自启动12$ sudo systemctl start docker$ sudo systemctl enable docker 配置阿里云镜像加速自行注册阿里云账号，获取自己的镜像加速地址：https://cr.console.aliyun.com/cn-hongkong/instances/mirrors 解决普通用户（非root)无权限使用docker命令问题docker进程使用Unix Socket而不是TCP端口。而默认情况下，Unix socket属于root用户，需要root权限才能访问。为了方便普通用户每次执行docker不需要加sudo,我们可以用以下方法给普通用户授权： docker守护进程启动的时候，会默认赋予名字为docker的用户组读写Unix socket的权限，因此只要创建docker用户组，并将当前用户加入到docker用户组中，那么当前用户就有权限访问Unix socket了，进而也就可以执行docker相关命令。 1234sudo groupadd docker #添加docker用户组sudo gpasswd -a $USER docker #将登陆用户加入到docker用户组中newgrp docker #更新用户组docker ps #测试docker命令是否可以使用sudo正常使用 Docker - Compose 安装从 Github 上下载它的二进制包来使用，最新发行的版本地址：https://github.com/docker/compose/releases。运行以下命令下载到usr/local/bin目录下： 1curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose 在国内通过curl有时经常超时，这时我们可以直接到github上找到我们的对应的版本下载二进制文件到本地，将其重命名为docker-compose , 并上传到/usr/local/bin 目录下。例如：https://github.com/docker/compose/releases/download/1.25.4/docker-compose-Linux-x86_64 运行以下命名，使下载的二进制文件具有可执行权限： 1sudo chmod +x /usr/local/bin/docker-compose 结束语Centos7 安装 Docker 就此完成，感谢支持！","categories":[{"name":"Linux","slug":"Linux","permalink":"http://linco.com/categories/Linux/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"},{"name":"后端","slug":"后端","permalink":"http://linco.com/tags/%E5%90%8E%E7%AB%AF/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://linco.com/categories/Linux/"}]},{"title":"Linux 查看内存及CPU使用情况","slug":"Linux 内存相关知识","date":"2021-05-16T07:09:00.846Z","updated":"2021-05-16T07:09:00.852Z","comments":true,"path":"2021/05/16/Linux 内存相关知识/","link":"","permalink":"http://linco.com/2021/05/16/Linux%20%E5%86%85%E5%AD%98%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","excerpt":"","text":"方式一：单独查看内存使用情况的命令1free -选项 选项：&emsp;&emsp;-b：以Byte为单位显示内存使用情况；&emsp;&emsp;-k：以KB为单位显示内存使用情况；&emsp;&emsp;-m：以MB为单位显示内存使用情况；&emsp;&emsp;-o：不显示缓冲区调节列；&emsp;&emsp;-s&lt;间隔秒数&gt;：持续观察内存使用状况；&emsp;&emsp;-t：显示内存总和列；&emsp;&emsp;-V：显示版本信息。 实例： 1234[linco@linco-centos7 ~]$ free -m total used free shared buff/cache availableMem: 991 224 166 6 599 604Swap: 2047 1 2046 其中第一列解释如下：&emsp;&emsp;Mem 内存的使用信息&emsp;&emsp;Swap 交换空间的使用信息 第一行解释如下：&emsp;&emsp;total：系统总的可用物理内存大小；&emsp;&emsp;used：已被使用的物理内存大小；&emsp;&emsp;free：空闲的物理内存大小；&emsp;&emsp;shared：当前已经废弃不用；&emsp;&emsp;buff/cache：被 buffer 和 cache 使用的物理内存大小；&emsp;&emsp;available：还可以被 应用程序 使用的物理内存大小&emsp;&emsp;其中total = used + free free 与 available 的区别：free 是真正尚未被使用的物理内存数量。available 是应用程序认为可用内存数量，available = free + buffer + cache (注：只是大概的计算方法) buff/cache 清理：&emsp;&emsp;Linux服务器运行一段时间后，由于其内存管理机制，会将暂时不用的内存转为buff/cache，这样在程序使用到这一部分数据时，能够很快的取出，从而提高系统的运行效率，所以这也正是linux内存管理中非常出色的一点，所以乍一看内存剩余的非常少，但是在程序真正需要内存空间时,也就是free内存不够时，linux内核就会回收 buffer 和 cache 的内存让出给程序使用，这样达到对内存的最充分利用，所以真正剩余的内存是free+buff/cache。 &emsp;&emsp;但是有些时候大量的缓存占据空间，这时候应用程序回去使用swap交换空间，从而使系统变慢，这时候需要手动去释放内存，释放内存的时候，首先执行命令 sync 将所有正在内存中的缓冲区写到磁盘中，其中包括已经修改的文件inode、已延迟的块I/O以及读写映射文件，从而确保文件系统的完整性。[sync命令的作用] 1234567#先执行syncsync#再根据自己需求选择以下命令执行echo 1 &gt; /proc/sys/vm/drop_caches #表示清除pagecache。 echo 2 &gt; /proc/sys/vm/drop_caches #表示清除回收slab分配器中的对象（包括目录项缓存和inode缓存）。slab分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的pagecache。 echo 3 &gt; /proc/sys/vm/drop_caches #表示清除pagecache和slab分配器中的缓存对象，即所有缓存。 &emsp;&emsp;如果执行以上echo x &gt; … 提示没有权限，我们除了加sudo之外还需要使用 “sh -c” 命令,它可以让 bash 将一个字串作为完整的命令来执行。&emsp;&emsp;因为重定向符号 “&gt;” 也是 bash 的命令。如果仅用“ sudo echo 1 &gt; /proc/sys/vm/drop_caches”只是让 echo 命令具有了 root 权限，但是没有让 “&gt;” 命令也具有root 权限，所以 bash 会认为这个命令没有写入信息的权权限。参考 正确做法如下： 12345sudo sh -c \"echo 1 &gt; /proc/sys/vm/drop_caches\"sudo sh -c \"echo 2 &gt; /proc/sys/vm/drop_caches\"sudo sh -c \"echo 3 &gt; /proc/sys/vm/drop_caches\" Swap 交换分区：&emsp;&emsp;交换空间是现代Linux系统中的第二种内存类型。交换空间的主要功能是当实际内存被填满，需要更多的空间时，用磁盘空间代替RAM内存。可以在常规文件系统或逻辑卷上使用一个或多个专用交换分区或交换文件。 如何为swap交换分区扩容：增加交换分区一般有两种方法，第一种是系统安装时进行设置的。方法1：使用分区：在安装OS时划分出专门的交换分区，空间大小要事先规划好，启动系统时自动进行mount。这种方法只能在安装OS时设定，一旦设定好不容易改变，除非重装系统。方法2：使用swapfile：步骤：1、创建swapfile：root权限下，创建swapfile，假设当前目录为”/“,执行如下命令： 12 dd if=/dev/zero of=swapfile bs=1024 count=128000#在根目录下创建了一个swapfile,名称为“swapfile”，大小为128M，也可以把文件输出到自己想要的任何目录中，放在根目录下比较好，不容易误破坏，放在其他目录下则不然了； 命令中选项解释：if：即输入文件,input file，of：即输出文件,output file，dev/zero：是Linux的一种特殊字符设备(输入设备)，可以用来创建一个指定长度用于初始化的空文件，如临时交换文件，该设备无穷尽地提供0，可以提供任何你需要的数目；bs=1024 ：单位数据块（block）同时读入/输出的块字节大小为1024 个字节即1KB；count=128000 ：数据块（block）数量为128000 ，即128000个1KB。（dd命令里的单位M表示1024*1024,k表示1024）。 2、将swapfile设置为swap空间 1mkswap swapfile 3、启用交换空间： 1swapon swapfile 至此增加交换空间的操作结束了，可以使用free命令查看swap空间大小是否发生变化；4、如果不再使用空间可以选择关闭交换空间: 1swapoff swapfile 使用这种方法在每次系统启动时都需要手动设置、开启swapfile，比较麻烦，解决方法：在 /etc/rc.d/rc.local 文件的末行下追加加以下内容：/sbin/swapon /swapfile保存后退出，这样在系统启动后，swap空间就会自动加载了。 方式二：查看内存及cpu使用情况的命令：1top 方式三：安装htop工具，查看更直观安装命令如下： 123456#Ubuntu 使用 apt-get 安装sudo apt-get install htop#Centos7 使用yum安装sudo yum -y install epel-release #先增加一个第三方的源，名字叫：EPELsudo yum -y install htop 安装完后，直接输入命令： 1htop 即可看到如下信息：","categories":[{"name":"Linux","slug":"Linux","permalink":"http://linco.com/categories/Linux/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://linco.com/categories/Linux/"}]},{"title":"K8S (Kubernetes) 集群搭建","slug":"Kubernetes 集群搭建","date":"2021-05-16T07:09:00.834Z","updated":"2021-05-16T07:09:00.834Z","comments":true,"path":"2021/05/16/Kubernetes 集群搭建/","link":"","permalink":"http://linco.com/2021/05/16/Kubernetes%20%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","excerpt":"","text":"本次教程，使用docker 18.09.9 和kubelet-1.16.4，要求centos7.6 以上版本 1.环境准备首先准备三台互通的机器(此处用的是虚拟机)，CPU必须2核以上，内存master节点最好2G及以上： 宿主机名称 IP CPU/内存 备注 linco-centos73 192.168.31.223 2核/2G master linco-centos71 192.168.31.222 2核/2G work node linco-centos7 192.168.31.221 2核/2G work node 1.1 设置主机名 及 ip 解析如果机器名为localhost,则可以执行以下命令修改hostname: 12# 修改 hostnamehostnamectl set-hostname your_host_name 配置hostname对应ip解析： 12345sudo vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.31.222 linco-centos72 #添加ip解析 1.2 关闭selinux查看selinux是否关闭 12[linco@linco-centos7 ~]$ getenforceEnforcing # 未关闭 设置临时关闭： 1setenforce 0 # 临时关闭 永久关闭： 12345678sudo vi /etc/sysconfig/selinux# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled # 修改为disabled 1.3 关闭 swap不关闭则k8s关闭过程会报错 临时关闭： 1sudo swapoff -a 永久关闭： 12345678910111213[linco@linco-centos73 ~]$ vi /etc/fstab## /etc/fstab# Created by anaconda on Tue Mar 31 21:42:56 2020## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root / xfs defaults 0 0UUID=218a58f8-c80e-4720-a454-a4982a7a261b /boot xfs defaults 0 0#/dev/mapper/centos-swap swap swap defaults 0 0 #注释掉此行~ 1.4 配置ip_forward转发修改ip_forward配置文件，0表示禁止数据包转发，将其修改为1表示允许 1echo \"1\" &gt; /proc/sys/net/ipv4/ip_forward 更新yum源下载Centos7的源和Docker源 12345678cd /etc/yum.repos.d #进到yum源目录ll # 查看当前源#下载所需源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repowget -P /etc/yum.repos.d/ http://mirrors.aliyun.com/repo/epel-7.repowget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 配置K8S源： 1234567[root@linco-centos7 yum.repos.d]# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo&gt; [kubernetes]&gt; name=Kubernetes&gt; baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64&gt; enabled=1&gt; gpgcheck=0&gt; EOF 刷新yum缓存： 1yum clean all &amp;&amp; yum makecache fast 2.安装环境2.1 安装Docker,使用版本18.09.9如果已经安装可跳过此步骤 1yum install docker-ce-18.09.9 docker-ce-cli-18.09.9 containerd.io -y k8s 运行要求docker 的–cgroup-driver=systemd 1234567sudo vi /etc/docker/daemon.json&#123; \"registry-mirrors\": [\"https://xxxxx.mirror.aliyuncs.com\"], \"exec-opts\":[\"native.cgroupdriver=systemd\"]&#125; 启动docker 并设置开机自启动 1systemctl enable docker &amp;&amp; systemctl start docker 2.2 安装K8S组件1sudo yum install -y kubelet-1.16.4 kubeadm-1.16.4 kubectl-1.16.4 设置开机启动： 1systemctl enable kubelet &amp;&amp; systemctl start kubelet 添加kubectl上下文到环境中： 1234[linco@linco-centos7 ~]$ echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bash_profile[linco@linco-centos7 ~]$ cd ~[linco@linco-centos7 ~]$ source .bash_profile#在home目录中，配置生效 内核参数修改： k8s 网络一般使用flannel，该网络需要设置内核参数bridge-nf-call-iptables=1添加参数配置文件： 12345sudo vi /etc/sysctl.d/k8s.conf#添加配置如下net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1 执行： 1sudo sysctl -p /etc/sysctl.d/k8s.conf 至此，环境准备安装工作完成！ 3.Master 节点初始化1sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.16.4 --pod-network-cidr=10.244.0.0/16 执行成功如图： 根据提示执行以下三条命令： 123[linco@linco-centos73 ~]$ mkdir -p $HOME/.kube[linco@linco-centos73 ~]$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[linco@linco-centos73 ~]$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 添加flannel 的网络:&emsp;&emsp;按照提示，我们需要为k8s配置一个pod network。由于国内网络不通的原因，此操作可能无法完成。我们可以使用定制的文件kube-flannel.yml来完成，上传文件到你的系统后，使用下面命令： 1kubectl apply -f kube-flannel.yml 查看集群： 123[linco@linco-centos73 ~]$ kubectl get nodesNAME STATUS ROLES AGE VERSIONlinco-centos73 Ready master 16m v1.16.4 4.Work 节点初始化&emsp;&emsp;在要加入的工作节点机器上，执行master 初始化时提示的join 语句，即加到master 的管辖内 1sudo kubeadm join 192.168.31.223:6443 --token 27qxe5.ye0jtk7o01miijma --discovery-token-ca-cert-hash sha256:227e470a2e1cdf2ad3a13923184a04db937bf78d419e44f54fd49464a72e7cd3 查看集群： 12345[linco@linco-centos73 ~]$ kubectl get nodesNAME STATUS ROLES AGE VERSIONlinco-centos7 Ready &lt;none&gt; 119s v1.16.4linco-centos72 Ready &lt;none&gt; 21h v1.16.4linco-centos73 Ready master 21h v1.16.4 至此，K8S集群搭建完成！ 相关问题：K８S搭建完，正常情况下主机重启kubelet也会自动重启，但是我安装之后重启主机，运行kubelet相关命令报如下错误： 12[linco@linco-centos73 ~]$ kubectl get nodesThe connection to the server 192.168.31.223:6443 was refused - did you specify the right host or port? 运行 systemctl status kubelet 命令查看 kubelet 的情况，发现 kubelet 没有启动： 123456789[linco@linco-centos73 ~]$ systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: activating (auto-restart) (Result: exit-code) since 二 2020-04-14 20:41:54 CST; 3s ago Docs: https://kubernetes.io/docs/ Process: 2060 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (co=exited, status=255) Main PID: 2060 (code=exited, status=255) 原因分析： 由于 K8s 必须保持全程关闭交换内存，而我安装是只是暂时关闭swap,并没有永久关闭。所以机器重启后，swap 还是会自动启用，从而导致 kubelet 无法启动。 解决方法： 使用通过修改vi /etc/fstab文件的方式永久关闭swap. kubelet 即可自己重启成功。 再次运行kubectl get nodes将显示正常。","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"Java并发 之 线程基础、线程之间的共享与协作 （一）","slug":"Java 并发之 线程基础、线程之间的共享与协作","date":"2021-05-16T07:09:00.806Z","updated":"2021-05-16T07:09:00.831Z","comments":true,"path":"2021/05/16/Java 并发之 线程基础、线程之间的共享与协作/","link":"","permalink":"http://linco.com/2021/05/16/Java%20%E5%B9%B6%E5%8F%91%E4%B9%8B%20%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E3%80%81%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B1%E4%BA%AB%E4%B8%8E%E5%8D%8F%E4%BD%9C/","excerpt":"","text":"什么是进程和线程进程&emsp;&emsp;进程是程序运行资源分配的最小单位。&emsp;&emsp;进程是一个在内存中运行的应用程序。每个进程都有自己独立的一块内存空间，一个进程可以有多个线程，比如在Windows系统中，一个运行的xx.exe就是一个进程。 线程&emsp;&emsp;线程是CPU 调度的最小单位,必须依赖于进程而存在。&emsp;&emsp;线程是进程中的一个执行任务（控制单元），负责当前进程中程序的执行。一个进程至少有一个线程，一个进程可以运行多个线程，多个线程可共享数据。 进程与线程的区别&emsp;&emsp;线程具有许多传统进程所具有的特征，故又称为轻型进程(Light—Weight Process)或进程元；而把传统的进程称为重型进程(Heavy—Weight Process)，它相当于只有一个线程的任务。在引入了线程的操作系统中，通常一个进程都有若干个线程，至少包含一个线程。 根本区别： 进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位 资源开销： 每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。 包含关系： 如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线（线程）共同完成的；线程是进程的一部分，所以线程也被称为轻权进程或者轻量级进程。 内存分配： 同一进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的 影响关系： 一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃整个进程都死掉。所以多进程要比多线程健壮。 执行过程： 每个独立的进程有程序运行的入口、顺序执行序列和程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制，两者均可并发执行 澄清并行和并发并发：&emsp;&emsp;指应用能够交替执行不同的任务,比如：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。 并行：&emsp;&emsp;指应用能够同时执行不同的任务，即：单位时间内，多个处理器或多核处理器同时处理多个任务，是真正意义上的“同时进行”。 串行：有n个任务，由一个线程按顺序执行。由于任务、方法都在一个线程执行所以不存在线程不安全情况，也就不存在临界区的问题。 并发和并行两者区别：一个是交替执行,一个是同时执行. 做一个形象的比喻： 并发 = 两个队列和一台咖啡机。 并行 = 两个队列和两台咖啡机。 串行 = 一个队列和一台咖啡机。 高并发编程的意义、好处和注意事项&emsp;&emsp;由于多核多线程的CPU 的诞生,多线程、高并发的编程越来越受重视和关注。多线程可以给程序带来如下好处。 (1)充分利用CPU 的资源 &emsp;&emsp;现在市面上没有CPU 的内核不使用多线程并发机制的,特别是服务器还不止一个CPU,如果还是使用单线程的技术做思路,明显就out 了。因为程序的基本调度单元是线程,并且一个线程也只能在一个CPU的一个核的一个线程跑,如果你是个i3 的CPU 的话,最差也是双核心4 线程的运算能力:如果是一个线程的程序的话,那是要浪费3/4 的CPU 性能:如果设计一个多线程的程序的话,那它就可以同时在多个CPU 的多个核的多个线程上跑,可以充分地利用CPU,减少CPU 的空闲时间,发挥它的运算能力,提高并发量。就像我们平时坐地铁一样,很多人坐长线地铁的时候都在认真看书,而不是为了坐地铁而坐地铁,到家了再去看书,这样你的时间就相当于有了两倍。这就是为什么有些人时间很充裕,而有些人老是说没时间的一个原因,工作也是这样,有的时候可以并发地去做几件事情,充分利用我们的时间,CPU 也是一样,也要充分利用。 (2)加快响应用户的时间 &emsp;&emsp;比如我们经常用的迅雷下载,都喜欢多开几个线程去下载,谁都不愿意用一个线程去下载,为什么呢?答案很简单,就是多个线程下载快啊。我们在做程序开发的时候更应该如此,特别是我们做互联网项目,网页的响应时间若提升1s,如果流量大的话,就能增加不少转换量。做过高性能web 前端调优的都知道,要将静态资源地址用两三个子域名去加载,为什么?因为每多一个子域名,浏览器在加载你的页面的时候就会多开几个线程去加载你的页面资源,提升网站的响应速度。多线程,高并发真的是无处不在。 (3)可以使你的代码模块化,异步化,简单化 &emsp;&emsp;例如我们实现电商系统，下订单和给用户发送短信、邮件就可以进行拆分，将给用户发送短信、邮件这两个步骤独立为单独的模块，并交给其他线程去执行。这样既增加了异步的操作，提升了系统性能，又使程序模块化,清晰化和简单化。多线程应用开发的好处还有很多,可以开发中慢慢体会它的魅力。 总结： 充分利用多核CPU的计算能力：通过并发编程的形式可以将多核CPU的计算能力发挥到极致，性能得到提升 方便进行业务拆分，提升系统并发能力和性能：在特殊的业务场景下，先天的就适合于并发编程。现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。面对复杂业务模型，并行程序会比串行程序更适应业务需求，而并发编程更能吻合这种业务拆分 。 并发编程的缺点：&emsp;&emsp;并发编程的目的就是为了能提高程序的执行效率，提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、线程安全、死锁等问题 多线程程序需要注意事项(1)线程之间的安全性 &emsp;&emsp;同一个进程里面的多线程是资源共享的,也就是都可以访问同一个内存地址当中的一个变量。例如:若每个线程中对全局变量、静态变量只有读操作,而无写操作,一般来说,这个全局变量是线程安全的:若有多个线程同时执行写操作,一般都需要考虑线程同步,否则就可能影响线程安全。 (2)线程之间的死锁 &emsp;&emsp;为了解决线程之间的安全性引入了Java 的锁机制,而一不小心就会产生Java线程死锁的多线程问题,因为不同的线程都在等待那些根本不可能被释放的锁,从而导致所有的工作都无法完成。假设有两个线程,分别代表两个饥饿的人,他们必须共享刀叉并轮流吃饭。他们都需要获得两个锁:共享刀和共享叉的锁。假如线程A 获得了刀,而线程B 获得了叉。线程A 就会进入阻塞状态来等待获得叉,而线程B 则阻塞来等待线程A 所拥有的刀。这只是人为设计的例子,但尽管在运行时很难探测到,这类情况却时常发生 (3)线程太多了会将服务器资源耗尽形成死机当机 &emsp;&emsp;线程数太多有可能造成系统创建大量线程而导致消耗完系统内存以及CPU的“过渡切换”,造成系统的死机,那么我们该如何解决这类问题呢?某些系统资源是有限的,如文件描述符。多线程程序可能耗尽资源,因为每个线程都可能希望有一个这样的资源。如果线程数相当大,或者某个资源的侯选线程数远远超过了可用的资源数则最好使用资源池。一个最好的示例是数据库连接池。只要线程需要使用一个数据库连接,它就从池中取出一个,使用以后再将它返回池中。资源池也称为资源库。多线程应用开发的注意事项很多。 Java 程序中怎么保证多线程的运行安全？并发编程三要素（线程的安全性问题体现在）： 原子性：原子，即一个不可再被分割的颗粒。原子性指的是一个或多个操作要么全部执行成功要么全部执行失败。 可见性：一个线程对共享变量的修改,另一个线程能够立刻看到。（synchronized,volatile） 有序性：程序执行的顺序按照代码的先后顺序执行。（处理器可能会对指令进行重排序） 出现线程安全问题的原因： 线程切换带来的原子性问题 缓存导致的可见性问题 编译优化带来的有序性问题 解决办法： JDK Atomic开头的原子类、synchronized、LOCK，可以解决原子性问题 synchronized、volatile、LOCK，可以解决可见性问题 Happens-Before 规则可以解决有序性问题 Java 创建新的执行线程 从JDK Thread源码中，可以看到真正创建新的执行线程有两种方法： 继承 Thread 类：定义一个Thread类的子类，重写run方法，将相关逻辑实现，run()方法就是线程要执行的业务逻辑方法创建自定义的线程子类对象调用子类实例的star()方法来启动线程 实现Runnable接口：定义Runnable接口实现类MyRunnable，并重写run()方法创建MyRunnable实例myRunnable，以myRunnable作为target创建Thead对象，该Thread对象才是真正的线程对象调用线程对象的start()方法 除此之外，实际上还有以下方法： 实现 Callable 接口 （java 5.0 开始提供的创建新线程的方法）创建实现Callable接口的类myCallable以myCallable为参数创建FutureTask对象将FutureTask作为参数创建Thread对象调用线程对象的start()方法 runnable 和 callable 的区别？相同点 都是接口 都可以编写多线程程序 都采用Thread.start()启动线程 主要区别 Runnable 接口 run 方法无返回值；Callable 接口 call 方法有返回值，是个泛型，和Future、FutureTask配合可以用来获取异步执行的结果 Runnable 接口 run 方法只能抛出运行时异常，且无法捕获处理；Callable 接口 call 方法允许抛出异常，可以获取异常信息 注： Callalbe接口支持返回执行结果，需要调用FutureTask.get()得到，此方法会阻塞主进程的继续往下执行，如果不调用不会阻塞。 如何才能让Java的线程安全停止&emsp;&emsp;暂停、恢复和停止操作对应在线程Thread 的API 就是suspend()、resume()和stop()。但是这些API 是过期的，也就是不建议使用的。不建议使用的原因主要有：以suspend()方法为例，在调用后，线程不会释放已经占有的资源（比如锁），而是占有着资源进入睡眠状态，这样容易引发死锁问题。同样，stop()方法在终结一个线程时不会保证线程的资源正常释放，通常是没有给予线程完成资源释放工作的机会，因此会导致程序可能工作在不确定状态下。 &emsp;&emsp;安全的中止则是其他线程通过调用某个线程A 的interrupt()方法对其进行中断操作, 中断好比其他线程对该线程打了个招呼，“A，你要中断了”，不代表线程A 会立即停止自己的工作，同样的A 线程完全可以不理会这种中断请求。因为java 里的线程是协作式的，不是抢占式的。线程通过检查自身的中断标志位是否被置为true 来进行响应，线程通过方法isInterrupted()来进行判断是否被中断，也可以调用静态方法Thread.interrupted()来进行判断当前线程是否被中断，不过Thread.interrupted()会同时将中断标识位改写为false。 注意： 处于死锁状态的线程无法被中断 [参考]：https://blog.csdn.net/ThinkWon/article/details/104863992","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Java并发编程","slug":"Java并发编程","permalink":"http://linco.com/tags/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"后端","slug":"后端","permalink":"http://linco.com/tags/%E5%90%8E%E7%AB%AF/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"Docker容器内部实现[真]全局代理","slug":"Docker容器内部实现[真]全局代理","date":"2021-05-16T07:09:00.805Z","updated":"2021-05-16T07:09:00.805Z","comments":true,"path":"2021/05/16/Docker容器内部实现[真]全局代理/","link":"","permalink":"http://linco.com/2021/05/16/Docker%E5%AE%B9%E5%99%A8%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0[%E7%9C%9F]%E5%85%A8%E5%B1%80%E4%BB%A3%E7%90%86/","excerpt":"","text":"最近学习Scrapy爬虫，有个特殊的需求，本身scrapy crawler程序的请求有挂着代理ip访问网站，另外scrapy所部署的服务器(A服务器)无法访问外网，需要通过挂另外一台机器(B服务器)的代理才能访问外网。 一开始在A服务器上通过http_proxy/https_proxy配置了系统代理，不过发现Scrapy发送请求的时候绕过了系统代理，直接用本机网络去发送请求，因此无法访问外网。 那么，如何实现真正的全局代理，让服务器A上的所有网络请求都经过代理服务器B呢？ 网上找到一种解决方案，使用 redsocks + iptables 可以实现将所有网络请求转发到指定的代理。 由于本人scrapy是使用Docker部署，所以本篇文章将以实现Docker容器内部全局代理来写。","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]},{"title":"Docker容器内部产生过多core.xxx文件导致容器奔溃解决方案[selenium]","slug":"Docker容器内部产生过多core.xxx文件导致容器奔溃解决方案[selenium]","date":"2021-05-16T07:09:00.803Z","updated":"2021-05-16T07:09:00.803Z","comments":true,"path":"2021/05/16/Docker容器内部产生过多core.xxx文件导致容器奔溃解决方案[selenium]/","link":"","permalink":"http://linco.com/2021/05/16/Docker%E5%AE%B9%E5%99%A8%E5%86%85%E9%83%A8%E4%BA%A7%E7%94%9F%E8%BF%87%E5%A4%9Acore.xxx%E6%96%87%E4%BB%B6%E5%AF%BC%E8%87%B4%E5%AE%B9%E5%99%A8%E5%A5%94%E6%BA%83%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88[selenium]/","excerpt":"","text":"&emsp;&emsp;最近生产环境使用docker部署的selenium chrome-node一直奔溃，无法重启。 &emsp;&emsp;通过在测试场运行一段时间后，发现docker容器下主目录不断生成core.xxx文件，而且占用空间相对比较大。于是可以推测生产环境正是因为core.xxx文件过多，导致容器内部空间被占满，因为奔溃。 &emsp;&emsp;那么core.xxx到底是什么文件?怎么产生的? 通过搜索发送core.xxx是linux系统在某些程序奔溃退出时生成的内核文件，一般用于调试。具体可以参考此篇文章Linux下core文件及使用 &emsp;&emsp;知道了core.xxx文件的知识，我们就可以通过限制core.xxx的产生来避免问题的产生, 具体有以下解决方案. 方法一：docker run 命令添加–ulimit core=0参数： 1ocker run --ulimit core=0 ...... 方法二：docker-compose 中通过添加 ulimits 参数限制，如下： 123456789chromenode1: image: ### container_name: chromenode1 restart: always ulimits: core: soft: 0 #软限制，应用可以随时修改，不能超过硬限制 hard: 0 #系统硬限制，只能 root 用户提高 ...... &emsp;&emsp;通过限制core.xxx文件的产生，重新部署后，进入容器内部查看，已经不再有core.xxx文件的产生,问题完美解决。","categories":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://linco.com/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Docker","slug":"Docker","permalink":"http://linco.com/tags/Docker/"}],"keywords":[{"name":"后端","slug":"后端","permalink":"http://linco.com/categories/%E5%90%8E%E7%AB%AF/"}]}]}